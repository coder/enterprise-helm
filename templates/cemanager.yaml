# The following describes the Kubernetes deployment of the Coder Enterprise
# manager. The general setup is as follows:
#   - A namespace contains all the relevant resources for Coder Enterprise.
#   - A deployment describes the configuration of the manager container.
#   - A service is created to route requests to the manager pod.
#   - A service account (along with a Role and Role binding) is created to grant
#     the manager the necessary permissions to administer environments.
#
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    coder.deployment: cemanager
  name: cemanager
  namespace: {{ .Release.Namespace | quote }}
spec:
  replicas: {{ .Values.cemanager.replicas }}
  strategy:
    rollingUpdate:
      maxSurge: "25%"
      maxUnavailable: "25%"
  selector:
    matchLabels:
      coder.deployment: cemanager
  template:
    metadata:
      labels:
        coder.deployment: cemanager
    spec:
      # coder:coder
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      restartPolicy: Always
      # terminationGracePeriodSeconds should be set to the upper bound for container rebuilds and creates.
      # 5 minutes
      terminationGracePeriodSeconds: 300
      # Start the manager with a specific service account
      # that only has permissions to what it needs in order to
      # administer pods.
      serviceAccountName: cemanager
{{- include "coder.serviceTolerations" . | indent 6 }}
      containers:
        - name: cemanager
          image: {{ .Values.cemanager.image | quote }}
          imagePullPolicy: {{ .Values.imagePullPolicy | quote }}
          ports:
            - name: tcp-cemanager
              containerPort: 8080
          # cemanager is a daemon service, no need to allocate a tty for it.
          tty: false
          env:
            - name: ASSETS_URL
            - name: HUMAN_LOG
              value: {{ .Values.logging.human | quote }}
            - name: JSON_LOG
              value: {{ .Values.logging.json | quote }}
            - name: STACKDRIVER_LOG
              value: {{ .Values.logging.stackdriver | quote }}
              # ENVBUILDER_IMAGE describes the image used to build
              # user images and populate them with coder assets
              # (such as code-server).
            - name: ENVBUILDER_IMAGE
              value: {{ .Values.envbuilder.image | quote }}
              # ENVBOX_IMAGE describes the image used to provide
              # additional features to users for running applications
              # such as dockerd and kubernetes.
            - name: ENVBOX_IMAGE
              value: {{ .Values.envbox.image | quote }}
            - name: DOCKERD_IMAGE
              value: {{ .Values.dockerd.image | quote}}
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: VERBOSE
              value: "true"
              # ENVIRONMENT_SERVICE_ACCOUNT is the service account to assign
              # to all user environments. It's primarily used to ensure
              # environments abide a pod security policy if one is set.
            - name: ENVIRONMENT_SERVICE_ACCOUNT
              value: "environments"
            - name: STORAGE_CLASS
              value: {{ .Values.storageClassName | quote }}
            - name: CLUSTER_DOMAIN_SUFFIX
              value: {{ .Values.clusterDomainSuffix | quote }}
{{- include "coder.environments.configMapEnv" . | indent 12 }}
{{- include "coder.namespaceWhitelist.env" . | indent 12 }}
{{- include "coder.devurls.hostEnv" . | indent 12 }}
{{- include "coder.postgres.env" . | indent 12 }}
          command:
            # Bash is needed to pass signals correctly.
            - /bin/bash
            - -c
            - |
              echo Waiting on db;
              /wait_postgres.sh --host="$DB_HOST" --port="$DB_PORT" -U "$DB_USER";
              echo Starting entrypoint.sh;
              # Pass signals down to the envmanager.
              exec /entrypoint.sh cemanager
          readinessProbe:
            exec:
              # The traditional /healthz endpoint is not used since
              # nginx runs its own health check for the ingress controller
              # and serving the api from '/' causes the manager's healthcheck
              # to be masked.
              command:
                - curl
                - -s
                - localhost:8080/cem-healthz
            initialDelaySeconds: 30
            failureThreshold: 12
            # Poll the manager every 20 seconds. If we fail 12 times (4
            # minutes) the manager is most likely in a bad spot. It's possible
            # for migrations to take extended periods of time, so we need to
            # account for that.
            periodSeconds: 20
          livenessProbe:
            exec:
              command:
                - curl
                - -s
                - localhost:8080/cem-healthz
            initialDelaySeconds: 30
            failureThreshold: 12
            # Poll the manager every 20 seconds. If we fail 12 times (4
            # minutes) the manager is most likely in a bad spot. It's possible
            # for migrations to take extended periods of time, so we need to
            # account for that.
            periodSeconds: 20
{{- include "coder.resources" .Values.cemanager.resources | indent 10 }}
{{- include "coder.volumeMounts" . | indent 10 }}
{{- include "coder.volumes" . | indent 6 }}
---
apiVersion: v1
kind: Service
metadata:
  name: cemanager
  namespace: {{ .Release.Namespace | quote }}
  annotations:
    beta.cloud.google.com/backend-config: '{"ports": {"8080":"proxy-config"}}'
spec:
  type: {{ .Values.serviceType | quote}}
  selector:
    coder.deployment: cemanager
  ports:
    - name: tcp-cemanager
      port: 8080
      protocol: TCP
---
# The service account for the manager.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cemanager
  namespace: {{ .Release.Namespace | quote }}
---
# The roles the manager needs cluster wide for
# checking available resources.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cemanager-{{ .Release.Namespace }}-cluster
rules:
  - apiGroups:
      - "" # indicates the core API group
    resources:
        # Lets us determine the cores/RAM available on a node
        # to avoid letting a user select an unschedulable number.
      - nodes
      - pods
        # Lets us determine what namespaces are available for
        # scheduling. The permissions for a namespace are handled
        # via regular namespaced Roles.
      - namespaces
    # We request read-only permissions at the cluster level to prevent
    # the product from making any suprising mutations.
    verbs:
      - get
      - list
---
# Bind the cluster role to the cemanager.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  namespace: {{ .Release.Namespace | quote }}
  name: cemanager-{{ .Release.Namespace }}-clusterbinding
subjects:
  - kind: ServiceAccount
    name: cemanager
    namespace: {{ .Release.Namespace | quote }}
roleRef:
  kind: ClusterRole
  name: cemanager-{{ .Release.Namespace }}-cluster
  apiGroup: rbac.authorization.k8s.io
{{- include "coder.cemanager.roles" . }}
{{- include "coder.cemanager.rolebindings" . }}
